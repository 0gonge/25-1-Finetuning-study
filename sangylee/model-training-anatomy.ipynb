{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.48.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: nvidia-ml-py3 in /opt/conda/lib/python3.10/site-packages (7.352.0)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.19.4)\n",
      "Requirement already satisfied: python_dotenv in /opt/conda/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0+cu124)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (75.6.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets accelerate nvidia-ml-py3 wandb python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "\t\"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "\t\"labels\": np.random.randint(0, 2, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB\")\n",
    "    \n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 442 MB\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 547 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.ones((1, 1)).to(\"cuda\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1835 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-large-uncased\").to(\"cuda\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfalconlee236\u001b[0m (\u001b[33mOptiMap\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/isang-yun/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import dotenv\n",
    "import os\n",
    "import gc\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "wandb.login(key=os.environ[\"WANDB_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"wandb\", #w&b logging on\n",
    "    \"eval_strategy\": \"no\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737707127.191031  181567 cuda_executor.cc:1004] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1737707127.193664  181350 service.cc:146] XLA service 0x5654693c1360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737707127.193691  181350 service.cc:154]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1737707127.194182  181350 se_gpu_pjrt_client.cc:897] Using BFC allocator.\n",
      "I0000 00:00:1737707127.194235  181350 gpu_helpers.cc:114] XLA backend allocating 11731746816 bytes on device 0 for BFCAllocator.\n",
      "I0000 00:00:1737707127.194269  181350 gpu_helpers.cc:154] XLA backend will use up to 3910582272 bytes on device 0 for CollectiveBFCAllocator.\n",
      "I0000 00:00:1737707127.194427  181350 cuda_executor.cc:1004] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/isang-yun/25-1-Finetuning-study/sangylee/wandb/run-20250124_082528-hk2rb9rk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/OptiMap/huggingface/runs/hk2rb9rk' target=\"_blank\">tmp</a></strong> to <a href='https://wandb.ai/OptiMap/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/OptiMap/huggingface' target=\"_blank\">https://wandb.ai/OptiMap/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/OptiMap/huggingface/runs/hk2rb9rk' target=\"_blank\">https://wandb.ai/OptiMap/huggingface/runs/hk2rb9rk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737707129.177616  181350 cuda_dnn.cc:530] Loaded cuDNN version 90100\n",
      "2025-01-24 08:27:14.080935: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 7.51GiB (8067496761 bytes) by rematerialization; only reduced to 9.54GiB (10239437064 bytes), down from 10.70GiB (11490555772 bytes) originally\n",
      "2025-01-24 08:29:25.613002: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 7.51GiB (8067496761 bytes) by rematerialization; only reduced to 9.71GiB (10429593364 bytes), down from 10.85GiB (11655448448 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 591.53\n",
      "Samples/second: 0.87\n",
      "GPU memory occupied: 11777 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>477148858023936.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>128</td></tr><tr><td>train_loss</td><td>0.72563</td></tr><tr><td>train_runtime</td><td>591.5288</td></tr><tr><td>train_samples_per_second</td><td>0.866</td></tr><tr><td>train_steps_per_second</td><td>0.216</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tmp</strong> at: <a href='https://wandb.ai/OptiMap/huggingface/runs/hk2rb9rk' target=\"_blank\">https://wandb.ai/OptiMap/huggingface/runs/hk2rb9rk</a><br> View project at: <a href='https://wandb.ai/OptiMap/huggingface' target=\"_blank\">https://wandb.ai/OptiMap/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250124_082528-hk2rb9rk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
    "result = trainer.train()\n",
    "print_summary(result)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/isang-yun/25-1-Finetuning-study/sangylee/wandb/run-20250124_083741-grcm530d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/OptiMap/huggingface/runs/grcm530d' target=\"_blank\">tmp</a></strong> to <a href='https://wandb.ai/OptiMap/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/OptiMap/huggingface' target=\"_blank\">https://wandb.ai/OptiMap/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/OptiMap/huggingface/runs/grcm530d' target=\"_blank\">https://wandb.ai/OptiMap/huggingface/runs/grcm530d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_xla/core/xla_model.py:1383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  xldata.append(torch.load(xbio))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1047.24\n",
      "Samples/second: 0.49\n",
      "GPU memory occupied: 11779 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/global_step</td><td>▁█</td></tr><tr><td>train/grad_norm</td><td>▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>477148858023936.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>512</td></tr><tr><td>train/grad_norm</td><td>29.10516</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8578</td></tr><tr><td>train_loss</td><td>0.85375</td></tr><tr><td>train_runtime</td><td>1047.2399</td></tr><tr><td>train_samples_per_second</td><td>0.489</td></tr><tr><td>train_steps_per_second</td><td>0.489</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tmp</strong> at: <a href='https://wandb.ai/OptiMap/huggingface/runs/grcm530d' target=\"_blank\">https://wandb.ai/OptiMap/huggingface/runs/grcm530d</a><br> View project at: <a href='https://wandb.ai/OptiMap/huggingface' target=\"_blank\">https://wandb.ai/OptiMap/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250124_083741-grcm530d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size=1, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
    "result = trainer.train()\n",
    "print_summary(result)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 발견 사항\n",
    "\n",
    "1epoch → 512개 데이터를 학습하는데 \n",
    "GPU 0 Memory Allocated (%)\t12349341696 → 12.34GB를 씀\n",
    "저 위에 있는 11330MB는 model load + cuda kernel이기 때문에 \n",
    "실제 학습할때는 대량 1GB정도 사용 \n",
    "그렇다면 이를 4 batch size가 아닌, 1 batch size를 쓰면 어떨까?\n",
    "위의 이론에 따르면 사용하는 메모리의 수가 줄어야 한다. → 왜냐하면 한번에 GPU에 올리는 데이터 양이 적어지기 때문\n",
    "거기에 학습 시간은 늘어나야함\n",
    "\n",
    "### 결과\n",
    "batch size => 4 일 때 \n",
    "* GPU 0 Memory Allocated (%)\t12351438848 -> 12.351GB 사용\n",
    "* 학습 시간 591.53 second\n",
    "batch size => 1 일 때 \n",
    "* GPU 0 Memory Allocated (%)\t12349341696 → 12.349GB를 씀\n",
    "* 학습 시간 1047.24 second\n",
    "\n",
    "\n",
    "실제로 배치 크기가 줄어나면 학습 시간은 늘어나고 메모리 사용량은 소폭 감소한 것을 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
